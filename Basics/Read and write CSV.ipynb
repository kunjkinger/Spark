{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c1f1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b97b4f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"CSVs\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f760b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+------+-------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+-------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
      "|          Timestamp|Age|Gender|      Country|state|self_employed|family_history|treatment|work_interfere|  no_employees|remote_work|tech_company|  benefits|care_options|wellness_program| seek_help| anonymity|        leave|mental_health_consequence|phys_health_consequence|   coworkers|supervisor|mental_health_interview|phys_health_interview|mental_vs_physical|obs_consequence|comments|\n",
      "+-------------------+---+------+-------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+-------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
      "|2014-08-27 11:29:31| 37|Female|United States|   IL|           NA|            No|      Yes|         Often|          6-25|         No|         Yes|       Yes|    Not sure|              No|       Yes|       Yes|Somewhat easy|                       No|                     No|Some of them|       Yes|                     No|                Maybe|               Yes|             No|      NA|\n",
      "|2014-08-27 11:29:37| 44|     M|United States|   IN|           NA|            No|       No|        Rarely|More than 1000|         No|          No|Don't know|          No|      Don't know|Don't know|Don't know|   Don't know|                    Maybe|                     No|          No|        No|                     No|                   No|        Don't know|             No|      NA|\n",
      "+-------------------+---+------+-------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+-------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- Timestamp: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- self_employed: string (nullable = true)\n",
      " |-- family_history: string (nullable = true)\n",
      " |-- treatment: string (nullable = true)\n",
      " |-- work_interfere: string (nullable = true)\n",
      " |-- no_employees: string (nullable = true)\n",
      " |-- remote_work: string (nullable = true)\n",
      " |-- tech_company: string (nullable = true)\n",
      " |-- benefits: string (nullable = true)\n",
      " |-- care_options: string (nullable = true)\n",
      " |-- wellness_program: string (nullable = true)\n",
      " |-- seek_help: string (nullable = true)\n",
      " |-- anonymity: string (nullable = true)\n",
      " |-- leave: string (nullable = true)\n",
      " |-- mental_health_consequence: string (nullable = true)\n",
      " |-- phys_health_consequence: string (nullable = true)\n",
      " |-- coworkers: string (nullable = true)\n",
      " |-- supervisor: string (nullable = true)\n",
      " |-- mental_health_interview: string (nullable = true)\n",
      " |-- phys_health_interview: string (nullable = true)\n",
      " |-- mental_vs_physical: string (nullable = true)\n",
      " |-- obs_consequence: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##option1 \n",
    "\n",
    "df = spark.read.csv(r\"C:/Users/Kunj.Kinger/Desktop/spark/python_project/data/sample.csv\",\n",
    "                   header=True)\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b95587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+------+-------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+-------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
      "|          Timestamp|Age|Gender|      Country|state|self_employed|family_history|treatment|work_interfere|  no_employees|remote_work|tech_company|  benefits|care_options|wellness_program| seek_help| anonymity|        leave|mental_health_consequence|phys_health_consequence|   coworkers|supervisor|mental_health_interview|phys_health_interview|mental_vs_physical|obs_consequence|comments|\n",
      "+-------------------+---+------+-------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+-------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
      "|2014-08-27 11:29:31| 37|Female|United States|   IL|           NA|            No|      Yes|         Often|          6-25|         No|         Yes|       Yes|    Not sure|              No|       Yes|       Yes|Somewhat easy|                       No|                     No|Some of them|       Yes|                     No|                Maybe|               Yes|             No|      NA|\n",
      "|2014-08-27 11:29:37| 44|     M|United States|   IN|           NA|            No|       No|        Rarely|More than 1000|         No|          No|Don't know|          No|      Don't know|Don't know|Don't know|   Don't know|                    Maybe|                     No|          No|        No|                     No|                   No|        Don't know|             No|      NA|\n",
      "+-------------------+---+------+-------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+-------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- Timestamp: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- self_employed: string (nullable = true)\n",
      " |-- family_history: string (nullable = true)\n",
      " |-- treatment: string (nullable = true)\n",
      " |-- work_interfere: string (nullable = true)\n",
      " |-- no_employees: string (nullable = true)\n",
      " |-- remote_work: string (nullable = true)\n",
      " |-- tech_company: string (nullable = true)\n",
      " |-- benefits: string (nullable = true)\n",
      " |-- care_options: string (nullable = true)\n",
      " |-- wellness_program: string (nullable = true)\n",
      " |-- seek_help: string (nullable = true)\n",
      " |-- anonymity: string (nullable = true)\n",
      " |-- leave: string (nullable = true)\n",
      " |-- mental_health_consequence: string (nullable = true)\n",
      " |-- phys_health_consequence: string (nullable = true)\n",
      " |-- coworkers: string (nullable = true)\n",
      " |-- supervisor: string (nullable = true)\n",
      " |-- mental_health_interview: string (nullable = true)\n",
      " |-- phys_health_interview: string (nullable = true)\n",
      " |-- mental_vs_physical: string (nullable = true)\n",
      " |-- obs_consequence: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##option 2\n",
    "df = spark.read.format('csv').option(key='header',value=True) \\\n",
    "    .load(path=r'C:/Users/Kunj.Kinger/Desktop/spark/python_project/data/sample.csv')\n",
    "\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec57c62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|Age|Gender|\n",
      "+---+------+\n",
      "| 32|  Male|\n",
      "| 40|  Male|\n",
      "+---+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Age=28, Gender='Male'), Row(Age=42, Gender='Male')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##option1 \n",
    "schema = StructType().add(field='Age',data_type=IntegerType()).add(field='Gender',data_type=StringType())\n",
    "\n",
    "df = spark.read.csv(r\"C:/Users/Kunj.Kinger/Desktop/spark/python_project/data/sample2.csv\",schema=schema,\n",
    "                   header=True)\n",
    "df.show(2)\n",
    "df.printSchema()\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070f29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2820dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43fca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e3bf8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DataFrameWriter in module pyspark.sql.readwriter:\n",
      "\n",
      "class DataFrameWriter(OptionUtils)\n",
      " |  DataFrameWriter(df)\n",
      " |  \n",
      " |  Interface used to write a :class:`DataFrame` to external storage systems\n",
      " |  (e.g. file systems, key-value stores, etc). Use :attr:`DataFrame.write`\n",
      " |  to access this.\n",
      " |  \n",
      " |  .. versionadded:: 1.4\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrameWriter\n",
      " |      OptionUtils\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, df)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  bucketBy(self, numBuckets, col, *cols)\n",
      " |      Buckets the output by the given columns.If specified,\n",
      " |      the output is laid out on the file system similar to Hive's bucketing scheme.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numBuckets : int\n",
      " |          the number of buckets to save\n",
      " |      col : str, list or tuple\n",
      " |          a name of a column, or a list of names.\n",
      " |      cols : str\n",
      " |          additional names (optional). If `col` is a list it should be empty.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Applicable for file-based data sources in combination with\n",
      " |      :py:meth:`DataFrameWriter.saveAsTable`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> (df.write.format('parquet')  # doctest: +SKIP\n",
      " |      ...     .bucketBy(100, 'year', 'month')\n",
      " |      ...     .mode(\"overwrite\")\n",
      " |      ...     .saveAsTable('bucketed_table'))\n",
      " |  \n",
      " |  csv(self, path, mode=None, compression=None, sep=None, quote=None, escape=None, header=None, nullValue=None, escapeQuotes=None, quoteAll=None, dateFormat=None, timestampFormat=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, charToEscapeQuoteEscaping=None, encoding=None, emptyValue=None, lineSep=None)\n",
      " |      Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n",
      " |              exists.\n",
      " |      \n",
      " |      compression : str, optional\n",
      " |          compression codec to use when saving to file. This can be one of the\n",
      " |          known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      " |          snappy and deflate).\n",
      " |      sep : str, optional\n",
      " |          sets a separator (one or more characters) for each field and value. If None is\n",
      " |          set, it uses the default value, ``,``.\n",
      " |      quote : str, optional\n",
      " |          sets a single character used for escaping quoted values where the\n",
      " |          separator can be part of the value. If None is set, it uses the default\n",
      " |          value, ``\"``. If an empty string is set, it uses ``u0000`` (null character).\n",
      " |      escape : str, optional\n",
      " |          sets a single character used for escaping quotes inside an already\n",
      " |          quoted value. If None is set, it uses the default value, ``\\``\n",
      " |      escapeQuotes : str or bool, optional\n",
      " |          a flag indicating whether values containing quotes should always\n",
      " |          be enclosed in quotes. If None is set, it uses the default value\n",
      " |          ``true``, escaping all values containing a quote character.\n",
      " |      quoteAll : str or bool, optional\n",
      " |          a flag indicating whether all values should always be enclosed in\n",
      " |          quotes. If None is set, it uses the default value ``false``,\n",
      " |          only escaping values containing a quote character.\n",
      " |      header : str or bool, optional\n",
      " |          writes the names of columns as the first line. If None is set, it uses\n",
      " |          the default value, ``false``.\n",
      " |      nullValue : str, optional\n",
      " |          sets the string representation of a null value. If None is set, it uses\n",
      " |          the default value, empty string.\n",
      " |      dateFormat : str, optional\n",
      " |          sets the string that indicates a date format. Custom date formats follow\n",
      " |          the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to date type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd``.\n",
      " |      timestampFormat : str, optional\n",
      " |          sets the string that indicates a timestamp format.\n",
      " |          Custom date formats follow the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to timestamp type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      " |      ignoreLeadingWhiteSpace : str or bool, optional\n",
      " |          a flag indicating whether or not leading whitespaces from\n",
      " |          values being written should be skipped. If None is set, it\n",
      " |          uses the default value, ``true``.\n",
      " |      ignoreTrailingWhiteSpace : str or bool, optional\n",
      " |          a flag indicating whether or not trailing whitespaces from\n",
      " |          values being written should be skipped. If None is set, it\n",
      " |          uses the default value, ``true``.\n",
      " |      charToEscapeQuoteEscaping : str, optional\n",
      " |          sets a single character used for escaping the escape for\n",
      " |          the quote character. If None is set, the default value is\n",
      " |          escape character when escape and quote characters are\n",
      " |          different, ``\\0`` otherwise..\n",
      " |      encoding : str, optional\n",
      " |          sets the encoding (charset) of saved csv files. If None is set,\n",
      " |          the default UTF-8 charset will be used.\n",
      " |      emptyValue : str, optional\n",
      " |          sets the string representation of an empty value. If None is set, it uses\n",
      " |          the default value, ``\"\"``.\n",
      " |      lineSep : str, optional\n",
      " |          defines the line separator that should be used for writing. If None is\n",
      " |          set, it uses the default value, ``\\\\n``. Maximum length is 1 character.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.csv(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  format(self, source)\n",
      " |      Specifies the underlying output data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      source : str\n",
      " |          string, name of the data source, e.g. 'json', 'parquet'.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.format('json').save(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  insertInto(self, tableName, overwrite=None)\n",
      " |      Inserts the content of the :class:`DataFrame` to the specified table.\n",
      " |      \n",
      " |      It requires that the schema of the :class:`DataFrame` is the same as the\n",
      " |      schema of the table.\n",
      " |      \n",
      " |      Optionally overwriting any existing data.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  jdbc(self, url, table, mode=None, properties=None)\n",
      " |      Saves the content of the :class:`DataFrame` to an external database table via JDBC.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      url : str\n",
      " |          a JDBC URL of the form ``jdbc:subprotocol:subname``\n",
      " |      table : str\n",
      " |          Name of the table in the external database.\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      properties : dict\n",
      " |          a dictionary of JDBC database connection arguments. Normally at\n",
      " |          least properties \"user\" and \"password\" with their corresponding values.\n",
      " |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Don't create too many partitions in parallel on a large cluster;\n",
      " |      otherwise Spark might crash your external database systems.\n",
      " |  \n",
      " |  json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None, lineSep=None, encoding=None, ignoreNullFields=None)\n",
      " |      Saves the content of the :class:`DataFrame` in JSON format\n",
      " |      (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n",
      " |      specified path.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      compression : str, optional\n",
      " |          compression codec to use when saving to file. This can be one of the\n",
      " |          known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      " |          snappy and deflate).\n",
      " |      dateFormat : str, optional\n",
      " |          sets the string that indicates a date format. Custom date formats\n",
      " |          follow the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to date type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd``.\n",
      " |      timestampFormat : str, optional\n",
      " |          sets the string that indicates a timestamp format.\n",
      " |          Custom date formats follow the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to timestamp type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      " |      encoding : str, optional\n",
      " |          specifies encoding (charset) of saved json files. If None is set,\n",
      " |          the default UTF-8 charset will be used.\n",
      " |      lineSep : str, optional\n",
      " |          defines the line separator that should be used for writing. If None is\n",
      " |          set, it uses the default value, ``\\n``.\n",
      " |      ignoreNullFields : str or bool, optional\n",
      " |          Whether to ignore null fields when generating JSON objects.\n",
      " |          If None is set, it uses the default value, ``true``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.json(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  mode(self, saveMode)\n",
      " |      Specifies the behavior when data or table already exists.\n",
      " |      \n",
      " |      Options include:\n",
      " |      \n",
      " |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      " |      * `overwrite`: Overwrite existing data.\n",
      " |      * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      " |      * `ignore`: Silently ignore this operation if data already exists.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.mode('append').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  option(self, key, value)\n",
      " |      Adds an output option for the underlying data source.\n",
      " |      \n",
      " |      You can set the following option(s) for writing files:\n",
      " |          * ``timeZone``: sets the string that indicates a time zone ID to be used to format\n",
      " |              timestamps in the JSON/CSV datasources or partition values. The following\n",
      " |              formats of `timeZone` are supported:\n",
      " |      \n",
      " |              * Region-based zone ID: It should have the form 'area/city', such as                   'America/Los_Angeles'.\n",
      " |              * Zone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or                  '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'.\n",
      " |      \n",
      " |              Other short names like 'CST' are not recommended to use because they can be\n",
      " |              ambiguous. If it isn't set, the current value of the SQL config\n",
      " |              ``spark.sql.session.timeZone`` is used by default.\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  options(self, **options)\n",
      " |      Adds output options for the underlying data source.\n",
      " |      \n",
      " |      You can set the following option(s) for writing files:\n",
      " |          * ``timeZone``: sets the string that indicates a time zone ID to be used to format\n",
      " |              timestamps in the JSON/CSV datasources or partition values. The following\n",
      " |              formats of `timeZone` are supported:\n",
      " |      \n",
      " |              * Region-based zone ID: It should have the form 'area/city', such as                   'America/Los_Angeles'.\n",
      " |              * Zone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or                  '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'.\n",
      " |      \n",
      " |              Other short names like 'CST' are not recommended to use because they can be\n",
      " |              ambiguous. If it isn't set, the current value of the SQL config\n",
      " |              ``spark.sql.session.timeZone`` is used by default.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  orc(self, path, mode=None, partitionBy=None, compression=None)\n",
      " |      Saves the content of the :class:`DataFrame` in ORC format at the specified path.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      partitionBy : str or list, optional\n",
      " |          names of partitioning columns\n",
      " |      compression : str, optional\n",
      " |          compression codec to use when saving to file. This can be one of the\n",
      " |          known case-insensitive shorten names (none, snappy, zlib, and lzo).\n",
      " |          This will override ``orc.compress`` and\n",
      " |          ``spark.sql.orc.compression.codec``. If None is set, it uses the value\n",
      " |          specified in ``spark.sql.orc.compression.codec``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> orc_df = spark.read.orc('python/test_support/sql/orc_partitioned')\n",
      " |      >>> orc_df.write.orc(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  parquet(self, path, mode=None, partitionBy=None, compression=None)\n",
      " |      Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      partitionBy : str or list, optional\n",
      " |          names of partitioning columns\n",
      " |      compression : str, optional\n",
      " |          compression codec to use when saving to file. This can be one of the\n",
      " |          known case-insensitive shorten names (none, uncompressed, snappy, gzip,\n",
      " |          lzo, brotli, lz4, and zstd). This will override\n",
      " |          ``spark.sql.parquet.compression.codec``. If None is set, it uses the\n",
      " |          value specified in ``spark.sql.parquet.compression.codec``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  partitionBy(self, *cols)\n",
      " |      Partitions the output by the given columns on the file system.\n",
      " |      \n",
      " |      If specified, the output is laid out on the file system similar\n",
      " |      to Hive's partitioning scheme.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str or list\n",
      " |          name of columns\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.partitionBy('year', 'month').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  save(self, path=None, format=None, mode=None, partitionBy=None, **options)\n",
      " |      Saves the contents of the :class:`DataFrame` to a data source.\n",
      " |      \n",
      " |      The data source is specified by the ``format`` and a set of ``options``.\n",
      " |      If ``format`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, optional\n",
      " |          the path in a Hadoop supported file system\n",
      " |      format : str, optional\n",
      " |          the format used to save\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      partitionBy : list, optional\n",
      " |          names of partitioning columns\n",
      " |      **options : dict\n",
      " |          all other string options\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.mode(\"append\").save(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  saveAsTable(self, name, format=None, mode=None, partitionBy=None, **options)\n",
      " |      Saves the content of the :class:`DataFrame` as the specified table.\n",
      " |      \n",
      " |      In the case the table already exists, behavior of this function depends on the\n",
      " |      save mode, specified by the `mode` function (default to throwing an exception).\n",
      " |      When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n",
      " |      the same as that of the existing table.\n",
      " |      \n",
      " |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      " |      * `overwrite`: Overwrite existing data.\n",
      " |      * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      " |      * `ignore`: Silently ignore this operation if data already exists.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          the table name\n",
      " |      format : str, optional\n",
      " |          the format used to save\n",
      " |      mode : str, optional\n",
      " |          one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`             (default: error)\n",
      " |      partitionBy : str or list\n",
      " |          names of partitioning columns\n",
      " |      **options : dict\n",
      " |          all other string options\n",
      " |  \n",
      " |  sortBy(self, col, *cols)\n",
      " |      Sorts the output in each bucket by the given columns on the file system.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col : str, tuple or list\n",
      " |          a name of a column, or a list of names.\n",
      " |      cols : str\n",
      " |          additional names (optional). If `col` is a list it should be empty.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> (df.write.format('parquet')  # doctest: +SKIP\n",
      " |      ...     .bucketBy(100, 'year', 'month')\n",
      " |      ...     .sortBy('day')\n",
      " |      ...     .mode(\"overwrite\")\n",
      " |      ...     .saveAsTable('sorted_bucketed_table'))\n",
      " |  \n",
      " |  text(self, path, compression=None, lineSep=None)\n",
      " |      Saves the content of the DataFrame in a text file at the specified path.\n",
      " |      The text files will be encoded as UTF-8.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      compression : str, optional\n",
      " |          compression codec to use when saving to file. This can be one of the\n",
      " |          known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      " |          snappy and deflate).\n",
      " |      lineSep : str, optional\n",
      " |          defines the line separator that should be used for writing. If None is\n",
      " |          set, it uses the default value, ``\\n``.\n",
      " |      \n",
      " |      The DataFrame must have only one column that is of string type.\n",
      " |      Each row becomes a new line in the output file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from OptionUtils:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DataFrameWriter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d26e4537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1| kunj|\n",
      "|  2|raman|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data =[(1,'kunj'),(2,'raman')]\n",
    "\n",
    "schema = ['id','name']\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "672bb996",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(r'C:\\Users\\Kunj.Kinger\\Desktop\\spark\\data\\writecsv',header=True,mode='append') #will store in parts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd22991d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  2|raman|\n",
      "|  2|raman|\n",
      "|  1| kunj|\n",
      "|  1| kunj|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(r'C:\\Users\\Kunj.Kinger\\Desktop\\spark\\data\\writecsv',header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb2f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
