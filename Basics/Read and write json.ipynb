{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bdf09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9644df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"JSON\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed7fc319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "|_corrupt_record|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "|              [|       null|      null|       null|      null|   null|\n",
      "|           null|        1.4|       0.2|        5.1|       3.5| setosa|\n",
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- petalLength: double (nullable = true)\n",
      " |-- petalWidth: double (nullable = true)\n",
      " |-- sepalLength: double (nullable = true)\n",
      " |-- sepalWidth: double (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#option 1\n",
    "df = spark.read.format('json').load(r'C:\\Users\\Kunj.Kinger\\Desktop\\spark\\data\\iris.json')\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e9df82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "|_corrupt_record|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "|              [|       null|      null|       null|      null|   null|\n",
      "|           null|        1.4|       0.2|        5.1|       3.5| setosa|\n",
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- petalLength: double (nullable = true)\n",
      " |-- petalWidth: double (nullable = true)\n",
      " |-- sepalLength: double (nullable = true)\n",
      " |-- sepalWidth: double (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#option2 \n",
    "df = spark.read.json(r'C:\\Users\\Kunj.Kinger\\Desktop\\spark\\data\\iris.json')\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b393e93b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\nSince Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().\n      ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KUNJ~1.KIN\\AppData\\Local\\Temp/ipykernel_9636/692412779.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\Kunj.Kinger\\Desktop\\spark\\data\\emp.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \"\"\"\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \nSince Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().\n      "
     ]
    }
   ],
   "source": [
    "df = spark.read.json(r'C:\\Users\\Kunj.Kinger\\Desktop\\spark\\data\\emp.json')\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21ea94a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1| kunj|200000|\n",
      "|  2|Raman|400000|\n",
      "+---+-----+------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(r'C:\\Users\\Kunj.Kinger\\Desktop\\spark\\data\\emp.json',multiLine=True)\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8486ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+-------+----------+------------+--------------------+-----------+------------+-----+-----+\n",
      "|             address|          attributes|         business_id|          categories|         city|               hours|is_open|  latitude|   longitude|                name|postal_code|review_count|stars|state|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+-------+----------+------------+--------------------+-----------+------------+-----+-----+\n",
      "|1616 Chapala St, ...|{null, null, null...|Pns2l4eNsfO8kk83d...|Doctors, Traditio...|Santa Barbara|                null|      0|34.4266787|-119.7111968|Abby Rappoport, L...|      93101|           7|  5.0|   CA|\n",
      "|87 Grasso Plaza S...|{null, null, null...|mpf3x-BjTdTEA3yCZ...|Shipping Centers,...|       Affton|{8:0-18:30, 0:0-0...|      1| 38.551126|  -90.335695|       The UPS Store|      63123|          15|  3.0|   MO|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+-------+----------+------------+--------------------+-----------+------------+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- address: string (nullable = true)\n",
      " |-- attributes: struct (nullable = true)\n",
      " |    |-- AcceptsInsurance: string (nullable = true)\n",
      " |    |-- AgesAllowed: string (nullable = true)\n",
      " |    |-- Alcohol: string (nullable = true)\n",
      " |    |-- Ambience: string (nullable = true)\n",
      " |    |-- BYOB: string (nullable = true)\n",
      " |    |-- BYOBCorkage: string (nullable = true)\n",
      " |    |-- BestNights: string (nullable = true)\n",
      " |    |-- BikeParking: string (nullable = true)\n",
      " |    |-- BusinessAcceptsBitcoin: string (nullable = true)\n",
      " |    |-- BusinessAcceptsCreditCards: string (nullable = true)\n",
      " |    |-- BusinessParking: string (nullable = true)\n",
      " |    |-- ByAppointmentOnly: string (nullable = true)\n",
      " |    |-- Caters: string (nullable = true)\n",
      " |    |-- CoatCheck: string (nullable = true)\n",
      " |    |-- Corkage: string (nullable = true)\n",
      " |    |-- DietaryRestrictions: string (nullable = true)\n",
      " |    |-- DogsAllowed: string (nullable = true)\n",
      " |    |-- DriveThru: string (nullable = true)\n",
      " |    |-- GoodForDancing: string (nullable = true)\n",
      " |    |-- GoodForKids: string (nullable = true)\n",
      " |    |-- GoodForMeal: string (nullable = true)\n",
      " |    |-- HairSpecializesIn: string (nullable = true)\n",
      " |    |-- HappyHour: string (nullable = true)\n",
      " |    |-- HasTV: string (nullable = true)\n",
      " |    |-- Music: string (nullable = true)\n",
      " |    |-- NoiseLevel: string (nullable = true)\n",
      " |    |-- Open24Hours: string (nullable = true)\n",
      " |    |-- OutdoorSeating: string (nullable = true)\n",
      " |    |-- RestaurantsAttire: string (nullable = true)\n",
      " |    |-- RestaurantsCounterService: string (nullable = true)\n",
      " |    |-- RestaurantsDelivery: string (nullable = true)\n",
      " |    |-- RestaurantsGoodForGroups: string (nullable = true)\n",
      " |    |-- RestaurantsPriceRange2: string (nullable = true)\n",
      " |    |-- RestaurantsReservations: string (nullable = true)\n",
      " |    |-- RestaurantsTableService: string (nullable = true)\n",
      " |    |-- RestaurantsTakeOut: string (nullable = true)\n",
      " |    |-- Smoking: string (nullable = true)\n",
      " |    |-- WheelchairAccessible: string (nullable = true)\n",
      " |    |-- WiFi: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- hours: struct (nullable = true)\n",
      " |    |-- Friday: string (nullable = true)\n",
      " |    |-- Monday: string (nullable = true)\n",
      " |    |-- Saturday: string (nullable = true)\n",
      " |    |-- Sunday: string (nullable = true)\n",
      " |    |-- Thursday: string (nullable = true)\n",
      " |    |-- Tuesday: string (nullable = true)\n",
      " |    |-- Wednesday: string (nullable = true)\n",
      " |-- is_open: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(r'C:/Users/Kunj.Kinger/Desktop/spark/data/yelp_business.json')\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d37a4241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+\n",
      "| id|   name|  salary|\n",
      "+---+-------+--------+\n",
      "|  1|   kunj|200000.0|\n",
      "|  2|  Raman|400000.0|\n",
      "|  3| fjdkha|  2000.0|\n",
      "|  4|fasdjkn|  4000.0|\n",
      "+---+-------+--------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType().add(field='id',data_type=IntegerType()).add(field='name',data_type=StringType()).add(field='salary',data_type=FloatType())\n",
    "\n",
    "df = spark.read.json(r'C:\\Users\\Kunj.Kinger\\Desktop\\spark\\data\\emps_json\\*.json',schema=schema,multiLine=True)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06e753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
